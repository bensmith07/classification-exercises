{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dccd2ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vectorized operations\n",
    "import numpy as np\n",
    "\n",
    "# for dataframe manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# for vizualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for statistical calculations\n",
    "import scipy.stats as stats\n",
    "\n",
    "# for obtaining stock datasets\n",
    "from pydataset import data\n",
    "\n",
    "# for manipulation of time data\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sklearn as sk\n",
    "\n",
    "# filter out warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# our own functions for accessing our sql database\n",
    "from env import get_db_url, user, password, host\n",
    "\n",
    "# our own scripts\n",
    "import acquire\n",
    "import prepare\n",
    "import explore\n",
    "\n",
    "# pandas display preferences\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "#pd.option_context('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e911f2e",
   "metadata": {},
   "source": [
    "#### 4. use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. Create a pandas dataframe, df_iris, from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc15bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data('iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a2842b",
   "metadata": {},
   "source": [
    "#### 4a.     print the first 3 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e495454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad54c07",
   "metadata": {},
   "source": [
    "#### 4b.     print the number of rows and columns (shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a7f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25179d",
   "metadata": {},
   "source": [
    "#### 4c.     print the column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5732184",
   "metadata": {},
   "source": [
    "#### 4d. print the data type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46fec03",
   "metadata": {},
   "source": [
    "#### 4e.     print the summary statistics for each of the numeric variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9033c72",
   "metadata": {},
   "source": [
    "#### 5. Read the Table1_CustDetails table from your spreadsheet exercises google sheet into a dataframe named df_google_sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eff871",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://docs.google.com/spreadsheets/d/1gb4xDK4WmoM0kBTOiurSNZzz3cQE1IVnRQ59YsUmTmw/export?format=csv#gid=1023018493'\n",
    "df_google_sheets = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_sheets.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5338d8bd",
   "metadata": {},
   "source": [
    "#### 5a. assign the first 100 rows to a new dataframe, df_google_sheets_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f49177",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_sheets_sample = df_google_sheets.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba417b0c",
   "metadata": {},
   "source": [
    "#### 5b. print the number of rows of your original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa819f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_sheets.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a9666",
   "metadata": {},
   "source": [
    "#### 5c. print the first five column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_google_sheets.columns[:5]:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e7c39a",
   "metadata": {},
   "source": [
    "#### 5d. print the column names that have a data type of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_google_sheets.dtypes[df_google_sheets.dtypes == 'object'].index:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a7393",
   "metadata": {},
   "source": [
    "#### 5e. compute the range for each of the numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = df_google_sheets.select_dtypes('number')\n",
    "ranges = ranges.max() - ranges.min()\n",
    "ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d6129",
   "metadata": {},
   "source": [
    "#### 6. Download your spreadsheet exercises google sheet as an excel file (File → Download → Microsoft Excel). Read the Table1_CustDetails worksheet into a dataframe named df_excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d352a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel = pd.read_excel('df_excel.xlsx', sheet_name='Table1_CustDetails')\n",
    "df_excel.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf031d0",
   "metadata": {},
   "source": [
    "#### 6a. assign the first 100 rows to a new dataframe, df_excel_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel_sample = df_excel.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c0ffc",
   "metadata": {},
   "source": [
    "#### 6b. print the number of rows of your original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efe489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ca2df",
   "metadata": {},
   "source": [
    "#### 6c. print the first 5 column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_excel.columns[:5]:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc34570e",
   "metadata": {},
   "source": [
    "#### 6d. print the column names that have a data type of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169baeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_google_sheets.select_dtypes(include=[object]).columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a3eaf",
   "metadata": {},
   "source": [
    "#### 6e. compute the range for each of the numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26444ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = df_google_sheets.select_dtypes(exclude=[object])\n",
    "ranges = ranges.max() - ranges.min()\n",
    "ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877450bf",
   "metadata": {},
   "source": [
    "#### 7. read the data from the given google sheet into a dataframe, df_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id = '1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g'\n",
    "sheet_name = 'train'\n",
    "\n",
    "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "\n",
    "df_google = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaced97",
   "metadata": {},
   "source": [
    "#### 7a. Print the first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ae54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b34d7",
   "metadata": {},
   "source": [
    "#### 7b. print the number of rows and columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a27919",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced3b55",
   "metadata": {},
   "source": [
    "#### 7c. print the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_google.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72440fd5",
   "metadata": {},
   "source": [
    "#### 7d. print the data type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50669f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea8759",
   "metadata": {},
   "source": [
    "#### 7e. print the summary statistics for each of the numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac55c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54e127",
   "metadata": {},
   "source": [
    "#### 7f. Print the unique values for each of your categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cc5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = list(df_google.nunique()[df_google.nunique() < 10].index)\n",
    "for i in range(df_google[categorical_columns].shape[1]):\n",
    "    print(f'{df_google[categorical_columns].iloc[:, i].name}: {(df_google[categorical_columns].iloc[:, i].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simpler way:\n",
    "categorical_df = df_google.loc[:,df_google.nunique()<10]\n",
    "for col in categorical_df.columns:\n",
    "    print(f'{col}: {(categorical_df[col].unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea59b0",
   "metadata": {},
   "source": [
    "### 1. Make a function named get_titanic_data that returns the titanic data from the codeup data science database as a pandas data frame. Obtain your data from the Codeup Data Science Database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0540042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from env import get_db_url, user, password, host\n",
    "\n",
    "def get_titanic_data():\n",
    "    \n",
    "    filename = 'titanic.csv'\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        print('Reading from local CSV...')\n",
    "        return pd.read_csv(filename)\n",
    "    \n",
    "    url = get_db_url('titanic_db')\n",
    "    sql = '''\n",
    "    SELECT * FROM passengers\n",
    "    '''\n",
    "    \n",
    "    print('No local file exists\\nReading from SQL database...')\n",
    "    df = pd.read_sql(sql, url)\n",
    "\n",
    "    print('Saving to local CSV... ')\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4808541",
   "metadata": {},
   "source": [
    "### 2. Make a function named get_iris_data that returns the data from the iris_db on the codeup data science database as a pandas data frame. The returned data frame should include the actual name of the species in addition to the species_ids. Obtain your data from the Codeup Data Science Database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571df461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iris_data():\n",
    "    \n",
    "    filename = 'iris.csv'\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        print('Reading from local CSV...')\n",
    "        return pd.read_csv(filename)\n",
    "        \n",
    "    url = get_db_url('iris_db')\n",
    "    sql = '''\n",
    "    SELECT *\n",
    "      FROM species\n",
    "      JOIN measurements USING(species_id);\n",
    "    '''\n",
    "    \n",
    "    print('No local file exists\\nReading from SQL database...')\n",
    "    df = pd.read_sql(sql, url)\n",
    "    \n",
    "    print('Saving to local CSV...')\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae07de6",
   "metadata": {},
   "source": [
    "### 3. Make a function named get_telco_data that returns the data from the telco_churn database in SQL. In your SQL, be sure to join all 4 tables together, so that the resulting dataframe contains all the contract, payment, and internet service options. Obtain your data from the Codeup Data Science Database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36690176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_telco_data():\n",
    "    \n",
    "    filename = 'telco_chun.csv'\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        print('Reading from local CSV...')\n",
    "        return pd.read_csv(filename)\n",
    "    \n",
    "    url = get_db_url('telco_churn')\n",
    "    sql = '''\n",
    "    SELECT * \n",
    "      FROM customers\n",
    "        JOIN contract_types USING(contract_type_id)\n",
    "        JOIN internet_service_types USING(internet_service_type_id)\n",
    "        JOIN payment_types USING(payment_type_id)\n",
    "    '''\n",
    "    \n",
    "    print('No local file exists\\nReading from SQL database...')\n",
    "    df = pd.read_sql(sql, url)\n",
    "    \n",
    "    print('Saving to local CSV...')\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e9052",
   "metadata": {},
   "source": [
    "### 4. Once you've got your get_titanic_data, get_iris_data, and get_telco_data functions written, now it's time to add caching to them. To do this, edit the beginning of the function to check for the local filename of telco.csv, titanic.csv, or iris.csv. If they exist, use the .csv file. If the file doesn't exist, then produce the SQL and pandas necessary to create a dataframe, then write the dataframe to a .csv file with the appropriate name. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06244c41",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e382d",
   "metadata": {},
   "source": [
    "### Using the Iris Data: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2278ee",
   "metadata": {},
   "source": [
    "#### 1. Use the function defined in acquire.py to load the iris data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_iris_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d56f1",
   "metadata": {},
   "source": [
    "#### 2. Drop the species_id and measurement_id columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1135180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['species_id', 'measurement_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb88c35",
   "metadata": {},
   "source": [
    "#### 3. Rename the species_name column to just species.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d5d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'species_name': 'species'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b0aa73",
   "metadata": {},
   "source": [
    "#### 4. Create dummy variables of the species name and concatenate onto the iris dataframe. (This is for practice, we don't always have to encode the target, but if we used species as a feature, we would need to encode it).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(df['species'], drop_first=True)\n",
    "df = pd.concat([df, dummy_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d2959",
   "metadata": {},
   "source": [
    "#### 5. Create a function named prep_iris that accepts the untransformed iris data, and returns the data with the transformations above applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683831a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_iris(df):\n",
    "    df = df.drop(columns=['species_id', 'measurement_id'])\n",
    "    df = df.rename(columns={'species_name': 'species'})\n",
    "    dummy_df = pd.get_dummies(df['species'], drop_first=True)\n",
    "    df = pd.concat([df, dummy_df], axis=1)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94031909",
   "metadata": {},
   "source": [
    "### Using the Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ba09c",
   "metadata": {},
   "source": [
    "#### 1. Use the function defined in acquire.py to load the Titanic data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eaf960",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b442f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1111ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.alone.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e24ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.alone[(df.parch == 0) & (df.sibsp == 0)].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad869ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.alone[(df.parch > 0) | (df.sibsp > 0)].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8efa371",
   "metadata": {},
   "source": [
    "#### 2. Drop any unnecessary, unhelpful, or duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5dbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows, if they exist:\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d5dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns\n",
    "\n",
    "# class gives the same info as pclass\n",
    "# embarked gives the same info as embarked_town\n",
    "# deck has too many missing values\n",
    "# age has too many missing values\n",
    "# alone gives duplicate info for the combination of parch and sibsp (see above)\n",
    "# passenger_id is simply an index\n",
    "\n",
    "df = df.drop(columns=['class', 'embarked', 'deck', 'age', 'alone', 'passenger_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95ff7b",
   "metadata": {},
   "source": [
    "#### 3. Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7293b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['sex', 'embark_town']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    dummy_df = pd.get_dummies(df[col],\n",
    "                              prefix=df[col].name,\n",
    "                              drop_first=True,\n",
    "                              dummy_na=False)\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    # drop original column\n",
    "    df = df.drop(columns=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e272ad85",
   "metadata": {},
   "source": [
    "#### 4. Create a function named prep_titanic that accepts the raw titanic data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_titanic(df):\n",
    "\n",
    "    # drop duplicate rows, if they exist:\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # drop unnecessary columns\n",
    "    df = df.drop(columns=['class', 'embarked', 'deck', 'age', 'alone', 'passenger_id'])\n",
    "\n",
    "    # encode categorical columbns with dummy variables then drop the original columns\n",
    "    categorical_columns = ['sex', 'embark_town']\n",
    "    for col in categorical_columns:\n",
    "        dummy_df = pd.get_dummies(df[col],\n",
    "                                  prefix=df[col].name,\n",
    "                                  drop_first=True,\n",
    "                                  dummy_na=False)\n",
    "        df = pd.concat([df, dummy_df], axis=1)\n",
    "        df = df.drop(columns=col)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a01c2e",
   "metadata": {},
   "source": [
    "### Using the Telco dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac068a6a",
   "metadata": {},
   "source": [
    "#### 1. Use the function defined in acquire.py to load the Telco data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad64a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_telco_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc9b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b9af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows, if present\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df9ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up total_charges column and cast as float\n",
    "df['total_charges'] = df.total_charges.replace(' ', np.nan).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da46494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing brand new customers\n",
    "df = df[df.tenure != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbbe836",
   "metadata": {},
   "source": [
    "#### 2. Drop any unnecessary, unhelpful, or duplicated columns. This could mean dropping foreign key columns but keeping the corresponding string values, for example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea45cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_id columns are simply foreign key columns that have corresponding string values\n",
    "# customer_id is a primary key that is not useful for our analysis\n",
    "df = df.drop(columns=['payment_type_id', 'internet_service_type_id', 'contract_type_id', 'customer_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367951f",
   "metadata": {},
   "source": [
    "#### 3. Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd3d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    dummy_df = pd.get_dummies(df[col],\n",
    "                              prefix=df[col].name,\n",
    "                              drop_first=True,\n",
    "                              dummy_na=False)\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    df = df.drop(columns=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441a086",
   "metadata": {},
   "source": [
    "#### 4. Create a function named prep_telco that accepts the raw telco data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f6f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_telco(df):\n",
    "    \n",
    "    # drop duplicate rows, if present\n",
    "    \n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # clean up total charges column and cast as float\n",
    "    df['total_charges'] = df.total_charges.replace(' ', np.nan).astype(float)\n",
    "    \n",
    "    # removing brand new customers\n",
    "    df = df[df.tenure != 0]\n",
    "    \n",
    "    # drop columns:\n",
    "    \n",
    "    # *_type_id columns are simply foreign key columns that have corresponding string values\n",
    "    # customer_id is a primary key that is not useful for our analysis\n",
    "    df = df.drop(columns=['payment_type_id', 'internet_service_type_id', 'contract_type_id', 'customer_id'])\n",
    "    \n",
    "    # encode categorical columns with dummy variables\n",
    "    \n",
    "    categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        dummy_df = pd.get_dummies(df[col],\n",
    "                                  prefix=df[col].name,\n",
    "                                  drop_first=True,\n",
    "                                  dummy_na=False)\n",
    "        df = pd.concat([df, dummy_df], axis=1)\n",
    "        df = df.drop(columns=col)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4052a5e",
   "metadata": {},
   "source": [
    "# Exploratory Analysis - Exercises Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de72e295",
   "metadata": {},
   "source": [
    "## Section 1 - Iris Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96fd23c",
   "metadata": {},
   "source": [
    "### 1. Acquire, Prepare and Split your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc37d1f",
   "metadata": {},
   "source": [
    "#### Acquire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ccde9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = acquire.get_iris_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5f309",
   "metadata": {},
   "source": [
    "#### Prepare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = prepare.prep_iris(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a596a",
   "metadata": {},
   "source": [
    "#### Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d28f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(iris, test_size=.2, random_state=123, stratify=iris.species)\n",
    "train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce7aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a6332",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaeaa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2284d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0055c9e1",
   "metadata": {},
   "source": [
    "### 2. Univariate Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba7cd1",
   "metadata": {},
   "source": [
    "#### For each measurement type (quantitative variable): create a histogram, boxplot, and compute descriptive statistics (using.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840347c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# identify the columns that contain data for quantitative variables\n",
    "quantitative_columns = [col for col in train.columns[train.dtypes == 'float64']]\n",
    "\n",
    "# for each of those columns:\n",
    "for col in quantitative_columns:\n",
    "        \n",
    "        # display a histogram of that column's distribution\n",
    "        sns.histplot(train[col], stat='proportion')\n",
    "        plt.show()\n",
    "        \n",
    "        # display a boxplot of that column's distribution\n",
    "        sns.boxplot(train[col])\n",
    "        plt.show()\n",
    "        \n",
    "        # display the summary statistics\n",
    "        print(pd.DataFrame(train[col].describe()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d24a2f",
   "metadata": {},
   "source": [
    "#### For each species (categorical variable): create a frequency table and a bar plot of those frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the columns that contain data for categorical variables\n",
    "categorical_columns = ['species']\n",
    "\n",
    "# for each of those columns\n",
    "for col in categorical_columns:\n",
    "    \n",
    "    # display a frequency table\n",
    "    print(pd.DataFrame(train[col].value_counts())\n",
    "          .rename(columns={col: f'{col}_counts'}))\n",
    "    \n",
    "    # display a bar plot of those frequencies\n",
    "    sns.countplot(data=train,\n",
    "                  x=col)\n",
    "    plt.title(f'{col}_counts')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba954c",
   "metadata": {},
   "source": [
    "#### Document takeaways and any actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7116f32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20238e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe9eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47d4fb17",
   "metadata": {},
   "source": [
    "### 3. Bivariate Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836fad84",
   "metadata": {},
   "source": [
    "#### Visualize each measurement type (y-axis) with the species variable (x-axis) using barplots, adding a horizontal line showing the overall mean of the metric (y-axis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa5014",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target = 'species'\n",
    "quantitative_columns = [col for col in train.columns[train.dtypes == 'float64']]\n",
    "\n",
    "for col in quantitative_columns:\n",
    "    sns.barplot(data=train,\n",
    "                x=target,\n",
    "                y=col)\n",
    "    plt.axhline(train[col].mean(), \n",
    "                ls='--', \n",
    "                color='black')\n",
    "    plt.xlabel(None)\n",
    "    plt.title(col, fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd619e7",
   "metadata": {},
   "source": [
    "#### For each measurement type, compute the descriptive statistics for each species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614aa0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative_columns = [col for col in train.columns[train.dtypes == 'float64']]\n",
    "target = 'species'\n",
    "line_break = ('-' * 62)\n",
    "\n",
    "for col in quantitative_columns:\n",
    "    print(col)\n",
    "    print(train.groupby(by=target)[col].describe())   \n",
    "    print(line_break)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2b38a4",
   "metadata": {},
   "source": [
    "#### For virginica & versicolor: Compare the mean petal_width using the Mann-Whitney test (scipy.stats.mannwhitneyu) to see if there is a significant difference between the two groups. Do the same for the other measurement types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b33d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "train_virginica = train[train.species == 'virginica']\n",
    "train_versicolor = train[train.species == 'versicolor']\n",
    "\n",
    "# identify the columns on which to conduct the test (those that contain data for quantitative variables)\n",
    "quantitative_columns = [col for col in train.columns[train.dtypes == 'float64']]\n",
    "\n",
    "# for each of the columns with quantitative variables:\n",
    "for col in quantitative_columns:\n",
    "    \n",
    "    # establish hypothesis and alpha level\n",
    "    H0 = f'mean {col} for train_virginica flowers = mean {col} for train_versicolor flowers'\n",
    "    H1 = f'mean {col} for train_virginica flowers != mean {col} for train_versicolor flowers'\n",
    "    alpha = .05\n",
    "\n",
    "    # conduct the test\n",
    "    u, p = stats.mannwhitneyu(train_virginica[col], train_versicolor[col])\n",
    "    \n",
    "    # display test info and results\n",
    "    line_break = ('\\n' + '=' * 102 + '\\n')\n",
    "    print(f'MANN-WHITNEY U TEST FOR: {col.upper()}')\n",
    "    print()\n",
    "    print(f'H0: {H0}')\n",
    "    print(f'H1: {H1}')\n",
    "    print()\n",
    "    print(f'u = {u}')\n",
    "    print(f'p = {p.round(4)}')\n",
    "    print()\n",
    "    if p < alpha:\n",
    "        print('RESULT: Reject H0\\n')\n",
    "        print(f'Since p < alpha:\\n')\n",
    "        print(f'we reject the null hypothesis that:\\n\\n\\t{H0}\\n')\n",
    "        print(f'and we proceed under the assumption that:\\n\\n\\t{H1}')\n",
    "    elif p > alpha:\n",
    "        print('RESULT: Fail to Reject H0\\n')\n",
    "        print(f'Since p > alpha:\\n')\n",
    "        print(f'we fail to reject the null hypothesis\\n')\n",
    "        print(f'and we proceed under the assumption that:\\n\\n\\t{H0}')\n",
    "    print(line_break)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07882f58",
   "metadata": {},
   "source": [
    "#### Document takeaways and any actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a1f547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53c4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce6686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d25ed8b3",
   "metadata": {},
   "source": [
    "### 4. Multivariate Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea57afb",
   "metadata": {},
   "source": [
    "#### Visualize the interaction of each measurement type with the others using a pairplot (or scatter matrix or something similar) and add color to represent species.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc53183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(train.drop(columns=['virginica', 'versicolor']), hue='species')\n",
    "g.map_upper(sns.scatterplot)\n",
    "g.map_lower(sns.kdeplot)\n",
    "g.map_diag(sns.kdeplot, legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aa8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.drop(columns=['versicolor', 'virginica']).corr().abs(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b458133",
   "metadata": {},
   "source": [
    "#### Visualize two numeric variables by means of the species. Hint: sns.relplot with hue or col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='petal_length',\n",
    "            y='petal_width',\n",
    "            data=train,\n",
    "            hue='species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57bac4",
   "metadata": {},
   "source": [
    "#### Create a swarmplot using a melted dataframe of all your numeric variables. The x-axis should be the variable name, the y-axis the measure. Add another dimension using color to represent species. Document takeaways from this visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_melt = train.drop(columns=['versicolor', 'virginica']).melt(id_vars='species')\n",
    "\n",
    "sns.swarmplot(data=train_melt,\n",
    "              x='variable',\n",
    "              y='value',\n",
    "              hue='species')\n",
    "plt.xlabel(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a5ab26",
   "metadata": {},
   "source": [
    "#### Ask a specific question of the data, such as: is the sepal area signficantly different in virginica compared to setosa? Answer the question through both a plot and using a mann-whitney or t-test. If you use a t-test, be sure assumptions are met (independence, normality, equal variance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f949253",
   "metadata": {},
   "source": [
    "Is the petal area significantly different in virginica compared to setosa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08519032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "\n",
    "train['petal_area'] = train.petal_width * train.petal_length\n",
    "\n",
    "train_virginica = train[train.species == 'virginica']\n",
    "train_setosa = train[train.species == 'setosa']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a9a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish hypotheses\n",
    "\n",
    "H0 = 'mean of petal_area for virginica flowers = mean of petal_area for setosa flowers'\n",
    "H1 = 'mean of petal_area for virginica flowers != mean of petal_area for setosa flowers'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a464537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate assumptions - independence\n",
    "# yes, they are independent (no reason to think they're not independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate assumptions - variance\n",
    "print('virginica variance: ', round(train_virginica.petal_area.var(), 2))\n",
    "print('setosa variance: ', round(train_setosa.petal_area.var(), 2))\n",
    "\n",
    "stat, p = stats.levene(train_virginica.petal_area, train_setosa.petal_area)\n",
    "print('levene test p-value: ', p)\n",
    "\n",
    "# they do not have equal variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate assumptions - normality\n",
    "\n",
    "plt.hist(train_virginica.petal_area)\n",
    "plt.title(f'virginica petal area n = {train_virginica.shape[0]}')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(train_setosa.petal_area)\n",
    "plt.title(f'setosa petal area n = {train_setosa.shape[0]}')\n",
    "plt.show()\n",
    "\n",
    "# virginica petal area appears approximately normal, with the bulk of observations \n",
    "# concentrated in the middle values, and tailedness on each end. \n",
    "\n",
    "# however, setosa appears heavily skewed to the right\n",
    "\n",
    "# in addition, we only have 28 observations for each category\n",
    "\n",
    "# therefore, we will not assume normality for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct the test - Mann Whitney U\n",
    "u, p = stats.mannwhitneyu(train_virginica.petal_area, train_setosa.petal_area)\n",
    "\n",
    "# display test info and results\n",
    "line_break = ('\\n' + '=' * 102 + '\\n')\n",
    "print(f'MANN-WHITNEY U TEST FOR: {col.upper()}')\n",
    "print()\n",
    "print(f'H0: {H0}')\n",
    "print(f'H1: {H1}')\n",
    "print()\n",
    "print(f'u = {u}')\n",
    "print(f'p = {p.round(4)}')\n",
    "print()\n",
    "if p < alpha:\n",
    "    print('RESULT: Reject H0\\n')\n",
    "    print(f'Since p < alpha:\\n')\n",
    "    print(f'we reject the null hypothesis that:\\n\\n\\t{H0}\\n')\n",
    "    print(f'and we proceed under the assumption that:\\n\\n\\t{H1}')\n",
    "elif p > alpha:\n",
    "    print('RESULT: Fail to Reject H0\\n')\n",
    "    print(f'Since p > alpha:\\n')\n",
    "    print(f'we fail to reject the null hypothesis\\n')\n",
    "    print(f'and we proceed under the assumption that:\\n\\n\\t{H0}')\n",
    "print(line_break)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b463f77",
   "metadata": {},
   "source": [
    "#### Document takeaways and any actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d13f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc52af55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5cc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59ae1e16",
   "metadata": {},
   "source": [
    "# Exploratory Analysis - Exercises Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279fcd7",
   "metadata": {},
   "source": [
    "### Explore your titanic dataset more completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af405ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = acquire.get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d79849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = prepare.prep_titanic(titanic, drop_after_encoding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(titanic, test_size=.2, random_state=123, stratify=titanic.survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae0384f",
   "metadata": {},
   "source": [
    "#### Determine drivers of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c55d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b46e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.display_uniques_1(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ebcff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d6fb82",
   "metadata": {},
   "source": [
    "Univariate Stat Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77571ef2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# identify the columns that contain data for quantitative variables\n",
    "quantitative_columns = ['fare', 'n_sibs_and_spouse', 'n_parents_and_children', 'family_size',]\n",
    "\n",
    "# for each of those columns:\n",
    "for col in quantitative_columns:\n",
    "        \n",
    "        # display a histogram of that column's distribution\n",
    "        sns.histplot(train[col], stat='proportion')\n",
    "        plt.show()\n",
    "        \n",
    "        # display a boxplot of that column's distribution\n",
    "        sns.boxplot(train[col])\n",
    "        plt.show()\n",
    "        \n",
    "        # display the summary statistics\n",
    "        print(pd.DataFrame(train[col].describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71beae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3836536",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# identify the columns that contain data for categorical variables\n",
    "categorical_columns = ['pclass', 'sex', 'alone', 'embark_town']\n",
    "\n",
    "# for each of those columns\n",
    "for col in categorical_columns:\n",
    "    \n",
    "    # display a frequency table\n",
    "    print(pd.DataFrame(train[col].value_counts())\n",
    "          .rename(columns={col: f'{col} counts'}))\n",
    "          \n",
    "    # display a bar plot of those frequencies\n",
    "    sns.countplot(data=train,\n",
    "                  x=col,)\n",
    "    plt.title(f'{col} counts')\n",
    "    plt.xlabel(None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029a2a5",
   "metadata": {},
   "source": [
    "####     Determine if certain columns should be dropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47cf1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e937bf0",
   "metadata": {},
   "source": [
    "####     Determine if it would be valuable to bin some numeric columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15f0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a25d61cc",
   "metadata": {},
   "source": [
    "####     Determine if it would be valuable to combine multiple columns into one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28adedd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5061e0c",
   "metadata": {},
   "source": [
    "#### Does it make sense to combine any features?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0aec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e3c7d3d",
   "metadata": {},
   "source": [
    "#### Do you find any surprises?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6d727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18f9f82c",
   "metadata": {},
   "source": [
    "#### Document any and all findings and takeaways in your notebook using markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec39bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

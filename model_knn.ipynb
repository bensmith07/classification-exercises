{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d41ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn as sk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from env import get_db_url, user, password, host\n",
    "\n",
    "import acquire\n",
    "import prepare\n",
    "import explore\n",
    "\n",
    "# pandas display preferences\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.precision', 3)\n",
    "#pd.option_context('display.max_rows', None)\n",
    "\n",
    "line_break = ('-' * 50)\n",
    "line_break_2 = ('\\n' + '=' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9471f14",
   "metadata": {},
   "source": [
    "# KNN Titanic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff7a7239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from local CSV...\n"
     ]
    }
   ],
   "source": [
    "df = acquire.get_titanic_data()\n",
    "df = prepare.prep_titanic(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af3b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'survived'\n",
    "positive = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2131eaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\t n = 498\n",
      "test\t n = 179\n",
      "validate n = 214\n"
     ]
    }
   ],
   "source": [
    "train, test, validate = prepare.train_test_validate_split(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea19fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate each sample into x and y\n",
    "x_train = train.drop(columns=target)\n",
    "y_train = train[target]\n",
    "\n",
    "x_validate = validate.drop(columns=target)\n",
    "y_validate = validate[target]\n",
    "\n",
    "x_test = test.drop(columns=target)\n",
    "y_test = test[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9d79f",
   "metadata": {},
   "source": [
    "### 1. Fit a K-Nearest Neighbors classifier to your training sample and transform (i.e. make predictions on the training sample)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94bedcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the classifer\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# fit the classifier to the training data\n",
    "clf = clf.fit(x_train, y_train)\n",
    "\n",
    "# make predictions to evaluate classifier performance on in-sample data\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "# establish baseline predictions and create dataframe to calculate performance\n",
    "train_results = pd.DataFrame()\n",
    "train_results['actual'] = train[target]\n",
    "train_results['baseline'] = train[target].mode()[0]\n",
    "train_results['predicted'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e44c66",
   "metadata": {},
   "source": [
    "### 2. Evaluate your results using the model score, confusion matrix, and classification report.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "373c0ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 0.84\n",
      "--------------------------------------------------\n",
      "Confusion Matrix: \n",
      " [[270  37]\n",
      " [ 45 146]]\n",
      "--------------------------------------------------\n",
      "Classification Report: \n",
      "                  0        1  accuracy  macro avg  weighted avg\n",
      "precision    0.857    0.798     0.835      0.827         0.834\n",
      "recall       0.879    0.764     0.835      0.822         0.835\n",
      "f1-score     0.868    0.781     0.835      0.824         0.835\n",
      "support    307.000  191.000     0.835    498.000       498.000\n"
     ]
    }
   ],
   "source": [
    "class_report = pd.DataFrame(classification_report(y_train, y_pred, output_dict=True))\n",
    "print(f'Model Score: {clf.score(x_train, y_train):.2f}')\n",
    "print(line_break)\n",
    "print('Confusion Matrix: \\n', confusion_matrix(y_train, y_pred))\n",
    "print(line_break)\n",
    "print('Classification Report: \\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd80883f",
   "metadata": {},
   "source": [
    "### 3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0192ac7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n",
      "Precision: 0.80\n",
      "Recall: 0.76\n",
      "F1 Score: 0.78\n",
      "Support 1: 191\n",
      "Support 0: 307\n",
      "\n",
      "True Postive Rate:\t0.29\n",
      "False Positive Rate:\t0.07\n",
      "True Negative Rate:\t0.54\n",
      "False Negative Rate:\t0.09\n"
     ]
    }
   ],
   "source": [
    "accuracy = clf.score(x_train, y_train)\n",
    "precision = sk.metrics.precision_score(y_train, y_pred, pos_label=positive)\n",
    "recall = sk.metrics.recall_score(y_train, y_pred, pos_label=positive)\n",
    "f1_score = sk.metrics.f1_score(y_train, y_pred, pos_label=positive)\n",
    "support_1 = int(y_train[y_train == 1].count())\n",
    "support_0 = int(y_train[y_train == 0].count())\n",
    "\n",
    "n=len(train)\n",
    "tp_rate = len(train_results[(train_results.predicted == positive) & (train_results.actual == positive)]) / n\n",
    "fp_rate = len(train_results[(train_results.predicted == positive) & (train_results.actual != positive)]) / n\n",
    "tn_rate = len(train_results[(train_results.predicted != positive) & (train_results.actual != positive)]) / n\n",
    "fn_rate = len(train_results[(train_results.predicted != positive) & (train_results.actual == positive)]) / n\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1_score:.2f}')\n",
    "print(f'Support 1: {support_1}')\n",
    "print(f'Support 0: {support_0}')\n",
    "print()\n",
    "print(f'True Postive Rate:\\t{tp_rate:.2f}')\n",
    "print(f'False Positive Rate:\\t{fp_rate:.2f}')\n",
    "print(f'True Negative Rate:\\t{tn_rate:.2f}')\n",
    "print(f'False Negative Rate:\\t{fn_rate:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bafc2b",
   "metadata": {},
   "source": [
    "### 4. Run through steps 2-4 setting k to 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4513bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 0.79\n",
      "--------------------------------------------------\n",
      "Confusion Matrix: \n",
      " [[271  36]\n",
      " [ 69 122]]\n",
      "--------------------------------------------------\n",
      "Classification Report: \n",
      "                  0        1  accuracy  macro avg  weighted avg\n",
      "precision    0.797    0.772     0.789      0.785         0.788\n",
      "recall       0.883    0.639     0.789      0.761         0.789\n",
      "f1-score     0.838    0.699     0.789      0.768         0.785\n",
      "support    307.000  191.000     0.789    498.000       498.000\n",
      "Accuracy: 0.79\n",
      "Precision: 0.77\n",
      "Recall: 0.64\n",
      "F1 Score: 0.70\n",
      "Support 1: 191\n",
      "Support 0: 307\n",
      "\n",
      "True Postive Rate:\t0.24\n",
      "False Positive Rate:\t0.07\n",
      "True Negative Rate:\t0.54\n",
      "False Negative Rate:\t0.14\n"
     ]
    }
   ],
   "source": [
    "# create the classifer\n",
    "clf = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# fit the classifier to the training data\n",
    "clf = clf.fit(x_train, y_train)\n",
    "\n",
    "# make predictions to evaluate classifier performance on in-sample data\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "# establish baseline predictions and create dataframe to calculate performance\n",
    "train_results = pd.DataFrame()\n",
    "train_results['actual'] = train[target]\n",
    "train_results['baseline'] = train[target].mode()[0]\n",
    "train_results['predicted'] = y_pred\n",
    "\n",
    "\n",
    "\n",
    "class_report = pd.DataFrame(classification_report(y_train, y_pred, output_dict=True))\n",
    "print(f'Model Score: {clf.score(x_train, y_train):.2f}')\n",
    "print(line_break)\n",
    "print('Confusion Matrix: \\n', confusion_matrix(y_train, y_pred))\n",
    "print(line_break)\n",
    "print('Classification Report: \\n', class_report)\n",
    "\n",
    "\n",
    "\n",
    "accuracy = clf.score(x_train, y_train)\n",
    "precision = sk.metrics.precision_score(y_train, y_pred, pos_label=positive)\n",
    "recall = sk.metrics.recall_score(y_train, y_pred, pos_label=positive)\n",
    "f1_score = sk.metrics.f1_score(y_train, y_pred, pos_label=positive)\n",
    "support_1 = int(y_train[y_train == 1].count())\n",
    "support_0 = int(y_train[y_train == 0].count())\n",
    "\n",
    "n=len(train)\n",
    "tp_rate = len(train_results[(train_results.predicted == positive) & (train_results.actual == positive)]) / n\n",
    "fp_rate = len(train_results[(train_results.predicted == positive) & (train_results.actual != positive)]) / n\n",
    "tn_rate = len(train_results[(train_results.predicted != positive) & (train_results.actual != positive)]) / n\n",
    "fn_rate = len(train_results[(train_results.predicted != positive) & (train_results.actual == positive)]) / n\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1_score:.2f}')\n",
    "print(f'Support 1: {support_1}')\n",
    "print(f'Support 0: {support_0}')\n",
    "print()\n",
    "print(f'True Postive Rate:\\t{tp_rate:.2f}')\n",
    "print(f'False Positive Rate:\\t{fp_rate:.2f}')\n",
    "print(f'True Negative Rate:\\t{tn_rate:.2f}')\n",
    "print(f'False Negative Rate:\\t{fn_rate:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb245f00",
   "metadata": {},
   "source": [
    "### 5. Run through setps 2-4 setting k to 20\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c37e81a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 0.73\n",
      "--------------------------------------------------\n",
      "Confusion Matrix: \n",
      " [[269  38]\n",
      " [ 94  97]]\n",
      "--------------------------------------------------\n",
      "Classification Report: \n",
      "                  0        1  accuracy  macro avg  weighted avg\n",
      "precision    0.741    0.719     0.735      0.730         0.732\n",
      "recall       0.876    0.508     0.735      0.692         0.735\n",
      "f1-score     0.803    0.595     0.735      0.699         0.723\n",
      "support    307.000  191.000     0.735    498.000       498.000\n",
      "Accuracy: 0.73\n",
      "Precision: 0.72\n",
      "Recall: 0.51\n",
      "F1 Score: 0.60\n",
      "Support 1: 191\n",
      "Support 0: 307\n",
      "\n",
      "True Postive Rate:\t0.19\n",
      "False Positive Rate:\t0.08\n",
      "True Negative Rate:\t0.54\n",
      "False Negative Rate:\t0.19\n"
     ]
    }
   ],
   "source": [
    "# create the classifer\n",
    "clf = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# fit the classifier to the training data\n",
    "clf = clf.fit(x_train, y_train)\n",
    "\n",
    "# make predictions to evaluate classifier performance on in-sample data\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "# establish baseline predictions and create dataframe to calculate performance\n",
    "train_results = pd.DataFrame()\n",
    "train_results['actual'] = train[target]\n",
    "train_results['baseline'] = train[target].mode()[0]\n",
    "train_results['predicted'] = y_pred\n",
    "\n",
    "\n",
    "\n",
    "class_report = pd.DataFrame(classification_report(y_train, y_pred, output_dict=True))\n",
    "print(f'Model Score: {clf.score(x_train, y_train):.2f}')\n",
    "print(line_break)\n",
    "print('Confusion Matrix: \\n', confusion_matrix(y_train, y_pred))\n",
    "print(line_break)\n",
    "print('Classification Report: \\n', class_report)\n",
    "\n",
    "\n",
    "\n",
    "accuracy = clf.score(x_train, y_train)\n",
    "precision = sk.metrics.precision_score(y_train, y_pred, pos_label=positive)\n",
    "recall = sk.metrics.recall_score(y_train, y_pred, pos_label=positive)\n",
    "f1_score = sk.metrics.f1_score(y_train, y_pred, pos_label=positive)\n",
    "support_1 = int(y_train[y_train == 1].count())\n",
    "support_0 = int(y_train[y_train == 0].count())\n",
    "\n",
    "n=len(train)\n",
    "tp_rate = len(train_results[(train_results.predicted == positive) & (train_results.actual == positive)]) / n\n",
    "fp_rate = len(train_results[(train_results.predicted == positive) & (train_results.actual != positive)]) / n\n",
    "tn_rate = len(train_results[(train_results.predicted != positive) & (train_results.actual != positive)]) / n\n",
    "fn_rate = len(train_results[(train_results.predicted != positive) & (train_results.actual == positive)]) / n\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1_score:.2f}')\n",
    "print(f'Support 1: {support_1}')\n",
    "print(f'Support 0: {support_0}')\n",
    "print()\n",
    "print(f'True Postive Rate:\\t{tp_rate:.2f}')\n",
    "print(f'False Positive Rate:\\t{fp_rate:.2f}')\n",
    "print(f'True Negative Rate:\\t{tn_rate:.2f}')\n",
    "print(f'False Negative Rate:\\t{fn_rate:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb71869",
   "metadata": {},
   "source": [
    "### 6. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e394bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dataframe to store model results\n",
    "model_results = pd.DataFrame(columns=['model_number', 'metric_type', 'sample_type', 'score'])\n",
    "\n",
    "# empty dataframe to store information about the model itself\n",
    "model_info = pd.DataFrame(columns=['model_number', 'K_neighbors'])\n",
    "######################################################################################\n",
    "# store baseline metrics\n",
    "\n",
    "model_number = 'baseline'\n",
    "\n",
    "# store info about the model\n",
    "dct = {'model_number': model_number,\n",
    "       'K_neighbors': 'N/A'}\n",
    "model_info = model_info.append(dct, ignore_index=True)\n",
    "\n",
    "# establish baseline predictions for train sample\n",
    "y_pred = baseline_pred = pd.Series([train[target].mode()[0]]).repeat(len(train))\n",
    "\n",
    "# get metrics\n",
    "dct = {'model_number': model_number, \n",
    "       'sample_type': 'train', \n",
    "       'metric_type': 'accuracy',\n",
    "       'score': sk.metrics.accuracy_score(y_train, y_pred)}\n",
    "model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "dct = {'model_number': model_number, \n",
    "       'sample_type': 'train', \n",
    "       'metric_type': 'precision',\n",
    "       'score': sk.metrics.precision_score(y_train, y_pred, pos_label=positive)}\n",
    "model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "dct = {'model_number': model_number, \n",
    "       'sample_type': 'train', \n",
    "       'metric_type': 'recall',\n",
    "       'score': sk.metrics.recall_score(y_train, y_pred, pos_label=positive)}\n",
    "model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "dct = {'model_number': model_number, \n",
    "       'sample_type': 'train', \n",
    "       'metric_type': 'f1_score',\n",
    "       'score': sk.metrics.f1_score(y_train, y_pred, pos_label=positive)}\n",
    "model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "# establish baseline predictions for validate sample\n",
    "y_pred = baseline_pred = pd.Series([train[target].mode()[0]]).repeat(len(validate))\n",
    "\n",
    "# get metrics\n",
    "dct = {'model_number': model_number, \n",
    "       'sample_type': 'validate', \n",
    "       'metric_type': 'f1_score',\n",
    "       'score': sk.metrics.f1_score(y_validate, y_pred, pos_label=positive)}\n",
    "model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "dct = {'model_number': model_number, \n",
    "       'sample_type': 'validate', \n",
    "       'metric_type': 'accuracy',\n",
    "       'score': sk.metrics.accuracy_score(y_validate, y_pred)}\n",
    "model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "dct = {'model_number': model_number, \n",
    "       'sample_type': 'validate', \n",
    "       'metric_type': 'precision',\n",
    "       'score': sk.metrics.precision_score(y_validate, y_pred, pos_label=positive)}\n",
    "model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "dct = {'model_number': model_number, \n",
    "       'sample_type': 'validate', \n",
    "       'metric_type': 'recall',\n",
    "       'score': sk.metrics.recall_score(y_validate, y_pred, pos_label=positive)}\n",
    "model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "# create models\n",
    "\n",
    "model_number = 1\n",
    "k_values = [5, 10, 20]\n",
    "\n",
    "for k in k_values:\n",
    "    \n",
    "    # store info about the model\n",
    "    dct = {'model_number': model_number,\n",
    "           'K_neighbors': k}\n",
    "    model_info = model_info.append(dct, ignore_index=True)\n",
    "    \n",
    "    # fit the classifier to the training sample and transform\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf = clf.fit(x_train, y_train)    \n",
    "\n",
    "    # results for train sample\n",
    "    y_pred = clf.predict(x_train)\n",
    "    \n",
    "    \n",
    "    # get metrics\n",
    "    dct = {'model_number': model_number, \n",
    "           'sample_type': 'train', \n",
    "           'metric_type': 'accuracy',\n",
    "           'score': sk.metrics.accuracy_score(y_train, y_pred)}\n",
    "    model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "    dct = {'model_number': model_number, \n",
    "           'sample_type': 'train', \n",
    "           'metric_type': 'precision',\n",
    "           'score': sk.metrics.precision_score(y_train, y_pred, pos_label=positive)}\n",
    "    model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "    dct = {'model_number': model_number, \n",
    "           'sample_type': 'train', \n",
    "           'metric_type': 'recall',\n",
    "           'score': sk.metrics.recall_score(y_train, y_pred, pos_label=positive)}\n",
    "    model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "    dct = {'model_number': model_number, \n",
    "           'sample_type': 'train', \n",
    "           'metric_type': 'f1_score',\n",
    "           'score': sk.metrics.f1_score(y_train, y_pred, pos_label=positive)}\n",
    "    model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "\n",
    "    # results for validate sample\n",
    "    y_pred = clf.predict(x_validate)\n",
    "    \n",
    "    # get metrics\n",
    "    dct = {'model_number': model_number, \n",
    "           'sample_type': 'validate', \n",
    "           'metric_type': 'f1_score',\n",
    "           'score': sk.metrics.f1_score(y_validate, y_pred, pos_label=positive)}\n",
    "    model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "    dct = {'model_number': model_number, \n",
    "           'sample_type': 'validate', \n",
    "           'metric_type': 'accuracy',\n",
    "           'score': sk.metrics.accuracy_score(y_validate, y_pred)}\n",
    "    model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "    dct = {'model_number': model_number, \n",
    "           'sample_type': 'validate', \n",
    "           'metric_type': 'precision',\n",
    "           'score': sk.metrics.precision_score(y_validate, y_pred, pos_label=positive)}\n",
    "    model_results = model_results.append(dct, ignore_index=True)\n",
    "\n",
    "    dct = {'model_number': model_number, \n",
    "           'sample_type': 'validate', \n",
    "           'metric_type': 'recall',\n",
    "           'score': sk.metrics.recall_score(y_validate, y_pred, pos_label=positive)}\n",
    "    model_results = model_results.append(dct, ignore_index=True)\n",
    "    \n",
    "    model_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b3e5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_results():\n",
    "    return model_results.pivot_table(columns='model_number', \n",
    "                                     index=('metric_type', 'sample_type'), \n",
    "                                     values='score',\n",
    "                                     aggfunc=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05967aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_number</th>\n",
       "      <th>K_neighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_number K_neighbors\n",
       "0     baseline         N/A\n",
       "1            1           5\n",
       "2            2          10\n",
       "3            3          20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd527e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_number</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric_type</th>\n",
       "      <th>sample_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <th>train</th>\n",
       "      <td>0.835</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <th>train</th>\n",
       "      <td>0.781</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <th>train</th>\n",
       "      <td>0.798</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <th>train</th>\n",
       "      <td>0.764</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model_number                 1      2      3  baseline\n",
       "metric_type sample_type                               \n",
       "accuracy    train        0.835  0.789  0.735     0.616\n",
       "f1_score    train        0.781  0.699  0.595     0.000\n",
       "precision   train        0.798  0.772  0.719     0.000\n",
       "recall      train        0.764  0.639  0.508     0.000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results[model_results.sample_type == 'train'].pivot_table(columns='model_number', \n",
    "                                                                 index=('metric_type', 'sample_type'), \n",
    "                                                                 values='score',\n",
    "                                                                 aggfunc=lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c6899",
   "metadata": {},
   "source": [
    "Each of the metrics tends to go down as we increase the value for K. Model #1, with K=5, performs best on in-sample data. As we incrase the number of neighbors compared to, those neighbors become less likely to be similar to our tested value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ee7fd",
   "metadata": {},
   "source": [
    "### 7.  Which model performs best on our out-of-sample data from validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3bbd7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_number</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric_type</th>\n",
       "      <th>sample_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <th>validate</th>\n",
       "      <td>0.748</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <th>validate</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <th>validate</th>\n",
       "      <td>0.675</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <th>validate</th>\n",
       "      <td>0.659</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model_number                 1      2      3  baseline\n",
       "metric_type sample_type                               \n",
       "accuracy    validate     0.748  0.743  0.710     0.617\n",
       "f1_score    validate     0.667  0.631  0.581     0.000\n",
       "precision   validate     0.675  0.701  0.652     0.000\n",
       "recall      validate     0.659  0.573  0.524     0.000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results[model_results.sample_type == 'validate'].pivot_table(columns='model_number', \n",
    "                                                                 index=('metric_type', 'sample_type'), \n",
    "                                                                 values='score',\n",
    "                                                                 aggfunc=lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fa3eb",
   "metadata": {},
   "source": [
    "For out of sample data, overall performance is still best for Model 1, though Model 2 (K=10) does slightly better on precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeb05d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_number</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric_type</th>\n",
       "      <th>sample_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">accuracy</th>\n",
       "      <th>train</th>\n",
       "      <td>0.835</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validate</th>\n",
       "      <td>0.748</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">f1_score</th>\n",
       "      <th>train</th>\n",
       "      <td>0.781</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validate</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">precision</th>\n",
       "      <th>train</th>\n",
       "      <td>0.798</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validate</th>\n",
       "      <td>0.675</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">recall</th>\n",
       "      <th>train</th>\n",
       "      <td>0.764</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validate</th>\n",
       "      <td>0.659</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model_number                 1      2      3  baseline\n",
       "metric_type sample_type                               \n",
       "accuracy    train        0.835  0.789  0.735     0.616\n",
       "            validate     0.748  0.743  0.710     0.617\n",
       "f1_score    train        0.781  0.699  0.595     0.000\n",
       "            validate     0.667  0.631  0.581     0.000\n",
       "precision   train        0.798  0.772  0.719     0.000\n",
       "            validate     0.675  0.701  0.652     0.000\n",
       "recall      train        0.764  0.639  0.508     0.000\n",
       "            validate     0.659  0.573  0.524     0.000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_model_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8dda3",
   "metadata": {},
   "source": [
    "But when we compare performance on train vs validate samples, Model 3, with K=20, tends to have the smallest difference in perforamnce between the two samples, and even performs slightly better on recall on the validate sample. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
